{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Supervised Model Example**\n",
    "#### **Description**: This dataset is a compiled set of Used Car Listings extracted from www.truecar.com. We will be creating a Decision Tree to predict the price of a given listing.\n",
    "##### **NOTE**: We are using a Decision Tree since the Categorical Features (e.g., make, model, trim) will greatly influence the price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1. Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling libraries\n",
    "import pandas as pd\n",
    "\n",
    "# data pre-processing libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# train and fit model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2. Preview Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_parquet('clean_car_listings.parquet')\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Perform Exploratory Data Analysis**\n",
    "#### For the purposes of this exercise, we will skip this step to keep thing simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Pre-Process Training Data**\n",
    "##### Computers can't read English, or any text for that matter. Thus, we need to convert all Categorical variables into numerical equivalents. We will use a technique known as Label Encoding to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "training_data['make'] = encoder.fit_transform(training_data['make'])\n",
    "training_data['model'] = encoder.fit_transform(training_data['model'])\n",
    "training_data['trim'] = encoder.fit_transform(training_data['trim'])\n",
    "training_data['mileage'] = encoder.fit_transform(training_data['mileage'])\n",
    "training_data['exterior_color'] = encoder.fit_transform(training_data['exterior_color'])\n",
    "training_data['interior_color'] = encoder.fit_transform(training_data['interior_color'])\n",
    "training_data['usage_type'] = encoder.fit_transform(training_data['usage_type'])\n",
    "training_data['city'] = encoder.fit_transform(training_data['city'])\n",
    "training_data['state'] = encoder.fit_transform(training_data['state'])\n",
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5. Split Training Data into Training and Testing Subsets**\n",
    "##### This step is used to ensure that the model has an adequate amount of training data to learn the general trends, yet enough testing data to validate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data.drop(columns=['price'])\n",
    "y = training_data['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6. Fit Training Data to Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7. View Feature Importance Chart**\n",
    "##### This step isn't necessary, though it provides insight for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From this visualization, we can see that \"model_year\", \"model\", and \"trim\" are the 3 most important features used to predict a listing's price, whereas \"num_accidents\", \"usage_type\", and \"num_owners\" are the 3 least important features to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 8. Evaluate Model Performance**\n",
    "##### To evaluate how well our model performed. We will analyze the following metrics:\n",
    "###### - **R-Sqaured**: This is a measure between 0 and 1 of how well the regression line fits the data.\n",
    "###### - **Max Depth**: This is a measure of how many levels our Decision Tree went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'R-squared: {round(model.score(X, y), 4)}')\n",
    "print(f'Max Depth: {model.get_depth()}')\n",
    "print(f'Prediciton Output: ${model.predict([[2016, 23, 124, 171, 96800, 1, 1, 1, 2, 2, 365, 37]])[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
