{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering Walkthrough**\n",
    "\n",
    "#### **Description**: In this Notebook, we'll discuss what Features and Feature Stores are, how they are used in Machine Learning, and various Feature Engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recap**\n",
    "##### Here is a quick review of the steps we took before Data Preprocessing:\n",
    "###### 1. **Data Sources**: We identified relevant data sources.\n",
    "###### 2. **Data Ingestion**: We got the data from the data sources identified.\n",
    "###### 3. **Data Storage**: We found a temporary storage repository for the data we ingested.\n",
    "###### 4. **Data Wrangling**: We cleaned the data to make it useable for analysis.\n",
    "###### 5. **Exploratory Data Analysis (EDA)**: We identified outliers, relationships between variables, and any missing values.\n",
    "###### 6. **Data Preprocessing**: We formatted the data in a way where the machine can make sense of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. What is Feature Engineering?**\n",
    "\n",
    "##### ***Feature Engineering*** is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning. \n",
    "###### (https://towardsdatascience.com/what-is-feature-engineering-importance-tools-and-techniques-for-machine-learning-2080b0269f10)\n",
    "\n",
    "##### When collecting data at the beginning of the Machine Learning Lifecycle, you'll likely have more than 100 Features to choose from. Although the majority of these may be helpful when predicting or classifying an observation, there will be quite a few that may create \"noise\" during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. What is a Feature Store?**\n",
    "\n",
    "##### **Watch the Following Video for a Short 2-Minute Summary:** https://www.youtube.com/watch?v=l6xfFYZAyns\n",
    "\n",
    "##### A ***feature store*** for machine learning is a platform that manages and provides access to both historical and live feature data and also provides support for creating point-in-time correct datasets from the historical feature data.\n",
    "\n",
    "###### (https://www.featurestore.org/what-is-a-feature-store)\n",
    "\n",
    "##### It is a central location where you can either create or update groups of features created from multiple different data sources, or create and update new datasets from those feature groups for training models or for use in applications that do not want to compute the features but just retrieve them when it needs them to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Feature Selection Techniques**\n",
    "##### In practice, Feature Selection is traditionally performed before Data Pre-processing.\n",
    "###### The following information was extracted from the following video: https://www.youtube.com/watch?v=7tW29jBceRw\n",
    "##### **Why Do we Perform Feature Selection?**\n",
    "##### Simply put, although modern computers are fast, they can quickly get bogged down when the concept of Big O Notation comes into play. Thus, Feature Selection provides the following advantages:\n",
    "- Shorten Model Training Times -> Improve Computational Efficiency\n",
    "- Reduce Generalization Errors in the Model by Removing Irrelevant Features\n",
    "- Improves the Predicitve Power of the Model and Reduces the Impact of Overfitting\n",
    "#### **Intrinsic Methods:**\n",
    "- Some algorithms have Feature Selection processes embedded. These simply put weights on the Features, allowing for less important features to be disregarded.\n",
    "- Decision Trees put weights on individual Features when finding the best split methods.\n",
    "- Random Forests uses an ensemble approach where it selects a random subset of Features to use in the tree and compares the relationship between them. It iterates through several combinations of Features until it gets an understanding of how well correlated each Feature is with all the other Features.\n",
    "- Regularization Models, such as Neural Networks, do something similar where they place an initial weight on all the features and then use a Loss Function to find the optimal weight for each Feature.\n",
    "#### **Filter Methods**\n",
    "- When using an algorithm that doesn't involve an Intrinsic Method, you can use Statistical Tests to identify the best subset of Features to use in your model.\n",
    "- Correlation Matrices can be used to visualize the relationships between the Target Feature and the Predictor Features.\n",
    "- When using a library such as SciKit-Learn, some algorithms provide a \"Feature Importance\" result set which uses a Statistical approach to identifying how much each individual Feature contributed to the outcome.\n",
    "- With Domain Knowledge, or knowledge applied to a specific field of study (e.g., healthcare, marketing/sales), you can use intuition to remove Features that you're confident won't affect the model whatsoever.\n",
    "#### **Wrapper Methods**\n",
    "- ML Engineers sometimes take an iterative approach where they first train the model on all the Features and then remove Features one by one until the best result is achieved.\n",
    "- On the other hand, ML Engineers also start with a single Feature and then add Featrues one at a time until the best result is achieved.\n",
    "- This is commonly used in Linear and Logistic Regression techniques where p-values are used to identify Feature Importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Engineering in Action**\n",
    "##### In the example below, our objective is to predict the price of a used car using the best set of Features we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# preview dataset\n",
    "\n",
    "training_data = pd.read_parquet('clean_car_listings.parquet')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Intrinsic Approach**\n",
    "###### In this example, we'll go through the MLOps process (let's assume we have already performed Data Wrangling and EDA). With the Intrinsic Approach, we'll make use of a Decision Tree to predict the price feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import plotly.express as px\n",
    "\n",
    "# preprocess data\n",
    "encoder = LabelEncoder()\n",
    "training_data['make'] = encoder.fit_transform(training_data['make'])\n",
    "training_data['model'] = encoder.fit_transform(training_data['model'])\n",
    "training_data['trim'] = encoder.fit_transform(training_data['trim'])\n",
    "training_data['exterior_color'] = encoder.fit_transform(training_data['exterior_color'])\n",
    "training_data['interior_color'] = encoder.fit_transform(training_data['interior_color'])\n",
    "training_data['usage_type'] = encoder.fit_transform(training_data['usage_type'])\n",
    "training_data['city'] = encoder.fit_transform(training_data['city'])\n",
    "training_data['state'] = encoder.fit_transform(training_data['state'])\n",
    "\n",
    "# specify predictor and target features\n",
    "X = training_data.drop(columns=['price'])\n",
    "y = training_data['price']\n",
    "\n",
    "# split dataset into Training and Testing Subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# fit training data to model\n",
    "used_car_model = DecisionTreeRegressor()\n",
    "used_car_model.fit(X_train, y_train)\n",
    "\n",
    "# create DataFrame with feature names and values, visualize feature importance chart\n",
    "features = pd.DataFrame({'value':used_car_model.feature_importances_}, index=[X.columns])\n",
    "features.reset_index(inplace=True)\n",
    "features.rename(columns={'level_0':'feature'}, inplace=True)\n",
    "px.bar(features.sort_values(by='value'), x='value', y='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using our Decision Tree's built-in Intrinsic Method of Feature Engineering, we can identify that **model_year, model**, and **trim** are the 3 most important Features, whereas **num_accidents, usage_type**, and **num_owners** are the three least important Features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Filter Approach**\n",
    "##### In this example, we'll do the same thing except use a Linear Regression model. To do this, we'll need to exclude all categorical features. Since Linear Regression is based on p-values we'll use the ***statsmodels*** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# read training dataset\n",
    "\n",
    "training_data = pd.read_parquet('clean_car_listings.parquet')\n",
    "\n",
    "# filter out categorical features\n",
    "categorical_feats = ['make', 'model', 'trim', 'exterior_color', 'interior_color', 'usage_type', 'city', 'state']\n",
    "training_data.drop(columns=categorical_feats, inplace=True)\n",
    "\n",
    "# view correlation matrix\n",
    "\n",
    "print(training_data.corr())\n",
    "print('')\n",
    "print('')\n",
    "# fit training data to model\n",
    "\n",
    "used_car_model = sm.OLS(training_data['price'], training_data.drop(columns=['price']))\n",
    "result = used_car_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using a Filter Approach, we could remove the \"num_accidents\" and \"num_owners\" Features as they have low correlations with the Target Feature. However, when training a Linear or Logistic Regression Model, p-values should always be used as they are the universal indicator of Statistical Significance.\n",
    "##### When we train the model and view the summary table, we can see the \"num_accidents\" and \"num_owners\" Features have p-values lower than 0.05, meaning they are statistically significant, whereas using the Correlation Matrix we would have likely chosen to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Wrapper Approach**\n",
    "##### In this example, we'll use the same dataset with the same target feature. However, this time, we'll explore the Forward Sequential and Backward Sequential Approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# read training dataset\n",
    "\n",
    "training_data = pd.read_parquet('clean_car_listings.parquet')\n",
    "\n",
    "# filter out categorical features\n",
    "categorical_feats = ['make', 'model', 'trim', 'exterior_color', 'interior_color', 'usage_type', 'city', 'state']\n",
    "training_data.drop(columns=categorical_feats, inplace=True)\n",
    "\n",
    "# view correlation matrix\n",
    "print('Correlation Matrix')\n",
    "print(training_data.corr())\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "'''\n",
    "Forward Sequential Approach. In this approach, we start with one Feature and add more features until we get the best result.\n",
    "'''\n",
    "\n",
    "# fit training data to model with 1 feature.\n",
    "used_car_model = sm.OLS(training_data['price'], training_data.drop(columns=['mileage', 'num_accidents', 'num_owners', 'price']))\n",
    "result = used_car_model.fit()\n",
    "summary = result.summary()\n",
    "results_as_html = summary.tables[1].as_html()\n",
    "summary = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "print('1 Feature Model Summary')\n",
    "print(summary)\n",
    "print('')\n",
    "\n",
    "# fit training data to model with 2 Features.\n",
    "used_car_model = sm.OLS(training_data['price'], training_data.drop(columns=['num_accidents', 'num_owners', 'price']))\n",
    "result = used_car_model.fit()\n",
    "summary = result.summary()\n",
    "results_as_html = summary.tables[1].as_html()\n",
    "summary = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "print('2 Feature Model Summary')\n",
    "print(summary)\n",
    "print('')\n",
    "\n",
    "# fit training data to model with 3 Features\n",
    "used_car_model = sm.OLS(training_data['price'], training_data.drop(columns=['num_owners', 'price']))\n",
    "result = used_car_model.fit()\n",
    "summary = result.summary()\n",
    "results_as_html = summary.tables[1].as_html()\n",
    "summary = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "print('3 Feature Model Summary')\n",
    "print(summary)\n",
    "print('')\n",
    "\n",
    "# fit training data to model with 4 Features\n",
    "used_car_model = sm.OLS(training_data['price'], training_data.drop(columns=['price']))\n",
    "result = used_car_model.fit()\n",
    "summary = result.summary()\n",
    "results_as_html = summary.tables[1].as_html()\n",
    "summary = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "print('4 Feature Model Summary')\n",
    "print(summary)\n",
    "print('')\n",
    "\n",
    "'''\n",
    "Backward Sequential Approach. In this approach, we start with all features and remove features as necessary.\n",
    "'''\n",
    "# fit training data to model with All Features\n",
    "used_car_model = sm.OLS(training_data['price'], training_data.drop(columns=['price']))\n",
    "result = used_car_model.fit()\n",
    "summary = result.summary()\n",
    "results_as_html = summary.tables[1].as_html()\n",
    "summary = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "print('4 Feature Model Summary')\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
